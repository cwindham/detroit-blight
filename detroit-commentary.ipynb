{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detroit Blight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Defining the Problem\n",
    "\n",
    "Urban blight is one of those nebulous concepts where everyone can picture it, but no one can define it exactly.  [One paper (Morckel, 2014)](http://www.thecyberhood.net/documents/papers/cd2014.pdf) found over 20 different definitions of the phenomenon.  The Department of Housing and Urban Development defines a structure as blighted when  [\"...it exhibits objectively determinable signs of deterioration sufficient to constitute a threat to human health, safety, and public welfare.\"](https://www.huduser.gov/portal/glossary/glossary_all.html#b)  Most of the definitions agree a property can be considered blighted if:\n",
    "- The property is owned by the local government, typically through a lien after tax foreclosure.\n",
    "- The property in disrepair, potentially to a dangerous degree.\n",
    "- The property is or may be demolished.\n",
    "\n",
    "We will use these three ideas as our working description of blight.  Here is an example of one such blighted building in Detroit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figs/blighted_building.png\" alt=\"blighted building\" style=\"width: 300px;\" />\n",
    "<small><i><a href=\"https://www.google.com/maps/@42.3359412,-83.0484953,3a,84.5y,212.04h,96.33t/data=!3m6!1e1!3m4!1sBpVLPQ_vUywLusKvOr6UWw!2e0!7i13312!8i6656!6m1!1e1\">figure source</a></i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blight in Detroit\n",
    "\n",
    "This paper will focus on one of the cities most affected by urban blight, Detroit, MI.  Detroit has an ongoing [open data initiative](https://data.detroitmi.gov/) to help the city make policy decisions.  My goal is to create a model that will predict whether a building is blighted or not based on public data available from the city.  Hopefully, this model will help the city to understand and identify the causes of blight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "Now, we need to get data that will help us to identify whether a building is blighted or not.  First, it is helpful to identify what constitutes a \"building\".  For our purposes, a building will be defined as being represented by an address, a property identification (parcel) number, and a set of coordinate boundaries delineating the property.  This information can be found in the [parcel map shapefile](https://data.detroitmi.gov/Property-Parcels/Parcel-Map/fxkw-udwf) and the [parcel points ownership](https://data.detroitmi.gov/Property-Parcels/Parcel-Points-Ownership/eijm-6nr4) file.\n",
    "\n",
    "Other data collected includes:\n",
    "- [detroit-311.csv](https://data.detroitmi.gov/Government/Improve-Detroit-Submitted-Issues/fwz3-w3yn):  Improve Detroit issues ([311 requests](https://en.wikipedia.org/wiki/3-1-1))\n",
    "- [detroit-blight-violations.csv](https://data.detroitmi.gov/Property-Parcels/Blight-Violations/teu6-anhh):  violations of [Detroit's blight code](https://www.municode.com/library/mi/detroit/codes/code_of_ordinances?nodeId=PTIIICICO_CH9BUBURE)\n",
    "- [detroit-demolition-permits.csv](https://data.detroitmi.gov/Government/Detroit-Demolitions/rv44-e9di):  building demolitions performed by the Detroit government\n",
    "- [detroit-fire.csv](https://data.detroitmi.gov/Public-Safety/2015-Fire-Data/g7tj-vvtd):  fire department calls\n",
    "\n",
    "All data was collected for the year 2015, except for the demolition permits which were collected 2015-present.  The rationale for this is that blighted buildings may wait a while to be demolished due to budget concerns, etc.  Therefore, buildings demolished in 2016 likely were blighted in 2015.  Also, as we will see, the dataset is imbalanced with many more non-blighted than blighted buildings.  Collecting through present gives us more positive examples to work with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling and Feature Engineering\n",
    "Recall our working description of blight:\n",
    "\n",
    "- The property is owned by the city, typically through a lien after tax foreclosure.\n",
    "- The property in disrepair, potentially to a dangerous degree.\n",
    "- The property is or may be demolished.\n",
    "\n",
    "This section will focus on generating features that can describe these criteria.\n",
    "\n",
    "## City Ownership\n",
    "The parcel ownership shapefile contains a column \"tax_status\" that lists a property's tax status e.g. taxable (private ownership), city owned, or county owned.  These tax statuses were cleaned and one-hot encoded.\n",
    "\n",
    "## Disrepair\n",
    "### Property value\n",
    "The value of a property is a good proxy for its condition.  In the parcel ownership file, the column \"SEV\" or sale estimated value was used to represent property value.  \n",
    "\n",
    "### Location\n",
    "Blight tends to vary geospatially neighborhood-to-neighborhood, so the latitude and longitude of the building were used as features as well.  The map below shows a hexplot of blighted buildings by location with arterial roads denoted by interior lines. As we will see, blight is far from evenly distributed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figs/blighted_buildings_hex.png\" alt=\"blighted building\"  style=\"height: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blight Violations\n",
    "Blight violations represent violations of Detroit's municipal code.  Some examples are:\n",
    "- Sec. 9-1-111. Graffiti and defacement; duty to remove.\n",
    "- Sec. 9-1-113. Minimum requirements for vacant buildings and structures\n",
    "\n",
    "As the above might suggest, blight violations are highly informative as to a property's physical status.  These data were cleaned and one-hot encoded on violation type.  The violations were then summed over location.\n",
    "\n",
    "### 311 Requests\n",
    "311 requests are calls for municipal service for issues like running water in a building, illegal dumping of waste, traffic signal issues, and clogged street drains.  Some of these features are clearly more informative than others, but since the machine learning model we will use, gradient boosting, has feature selection baked-in and is quite robust to overfitting, all of the 311 request types were considered.  As before, data was cleaned and one-hot encoded on issue type and summed by number of occurences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fire Department Calls\n",
    "As one might expect, fires are one of the leading causes of buildings becoming irreparably damaged.  If the owner cannot afford to replace the house, then these fire-damaged buildings are at a high risk of becoming blighted.  Also included in this file are other call types, like rubbish fires, that can be indicative of the property's condition.\n",
    "\n",
    "## Demolitions:  Defining Positive Examples\n",
    "The demolition file represents demolitions done by the local government of Detroit.  Since blight far outnumbers all other reasons for demolition in Detroit, the demolished buildings were assumed to be blighted.  Data was cleaned to remove incorrect parcel numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test Sets:  Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After cleaning and processing, it was necessary to somehow aggregate these data with the filtered parcel ownership list.  The blight violations and demolitions were joined to the parcel file on parcel number.  The 311 issues and fire department calls required a bit more processing first.\n",
    "\n",
    "311 requests are not necessarily specific to a building, so these data only contain latitude and longitude coordinates as a location identifier.  As such, the data was spatially joined using point-in-polygon tests on the coordinate data and building parcel shapefiles.  Computation was sped up using an algorithm called rtree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Joins with R-trees\n",
    "\n",
    "[*R-trees*](https://en.wikipedia.org/wiki/R-tree) are a way to perform containment tests on spatial objects in *O*(log *n*) time, not including construction of the tree.  They consist of [minimum spanning rectangles](https://en.wikipedia.org/wiki/Minimum_bounding_rectangle) placed around the more complicated polygonal parcel  shapes.  From these rectangles, the algorithm then recursively draws larger and larger minimum spanning rectangles bounding larger and larger groupings of objects.  These rectangles are axis-aligned with the coordinate system being used.  In our case, the x-dimension represents projected longitude and the y-dimension represents projected latitude.\n",
    "\n",
    "Rectangles are used because of their computationally efficient point-in-polygon tests, but this comes with one caveat:  for precise point-in polygon tests, the original polygon still must be tested because the minimum spanning rectangle might cover area not covered by the polygon itself.  In this case, a point can be inside the rectangle, but not in the polygon as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](files/figs/bounding_rectangle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the point could be located within multiple spanning rectangles, in which case all of the intersecting rectangles' polygons must be iterated over, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](files/figs/two_rectangles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the 311 issues, the fire data did not come with nice coordinate data.  In fact, I tried all of the common projections for Michigan (StatePlane, GeoRef, etc.) and couldn't find one that played nice with the fire coordinates. As such, joins on the one-hot encoded incident type and the building list were done using the address.  *A lot* of cleaning and standardization had to be done on the addresses before they were usable, but they eventually got there. See the data processing file for exact details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test split\n",
    "Since the positive (blighted) and negative (non-blighted) classes were *highly* imbalanced (0.02 to 0.98, respectively, [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) was used to split the data.  The two classes were proportionately allocated into the training and test datasets using an 80/20 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Random Forest\n",
    "\n",
    "## Deciding on a Classification Algorithm\n",
    "All told, the final model had ~385,000 unique data points and 279 distinct features.  Some were one-hot encodings of class feature, some were sums of occurences, and others, like sale value, latitude, and longitude, were continuous.  The respective ranges of these were (0 or 1), (0 to ~5, by discrete amounts), (0 to ~1e7), and (42.25 to 42.45 and -82.91 to -83.23). Given this mix of variable types, ranges, and the high-dimensional feature space I decided to use a [Random Forest Classifier(RFC)](https://en.wikipedia.org/wiki/Random_forest) since the algorithm handles this type of heterogeneous data well and is quite robust to overfitting.  Random forests also have the useful property of being able to weight classes differently. In this case, positive examples will receive a 50x weighting to account for the 50:1 imbalance between negative and positive classes.\n",
    "\n",
    "RFCs are an example of a bagging([Bootstrap AGGregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating) algorithm where decision trees are fit on a sample drawn with replacement.  This practice increases the bias of the forest (with respect to the bias of a single non-random tree), but the subsequent averaging of individual tree predictions lowers the variance enough that overall model performance of the forest is generally superior to that of a single tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Random forests give you several hyperparameters that can be adjusted to improve algorithm performance.  We will focus on three (given is the scikit-learn implementation names):\n",
    "- *n_estimators*: the number of trees in the model. Generally, setting this value as high as computationally possible is optimal, provided other hyperparameters are adjusted to prevent overfitting.\n",
    "\n",
    "\n",
    "- *min_samples_leaf*: how many examples are needed to produce a leaf node.  Increasing this value helps to reduce overfitting and increases robustness to outliers.\n",
    "\n",
    "\n",
    "- *max_features*: what percentage of features to fit on each tree i.e. randomly subsampling the features.  Smaller values help to increase bias and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, best practice is to first select an n_estimators as large as computationally possible.  Then, perform a [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) over likely values of the rest of the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score: Analyzing Model Performance on Unbalanced Classes\n",
    "\n",
    "As stated before, this dataset is imbalanced, with only 2% of the data belonging to the minority blighted class.  A trivial classifier that predicted all buildings as non-blighted would have an accuracy of 98%, not too bad.  In practice, evaluating the model on accuracy would likely lead to a model that would only predict as blighted buildings that it was highly confident on.  We could say that such a model would have [high precision and low recall](https://en.wikipedia.org/wiki/Precision_and_recall).  Using [AUROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) as a metric would not work either since the data is imbalanced and AUROC only takes into account true and false positives. One way of getting around this issue is to use [F1-score](https://en.wikipedia.org/wiki/F1_score), the harmonic mean of precision and recall.  As the harmonic mean, F1-score requires both precision and recall to be high to have good performance.\n",
    "\n",
    "### Cross-Validation\n",
    "During the grid search, mean F1-score on the <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation.html>5-fold stratified cross-validation</a> of the training set was used as the evaluation metric.  Optimal parameters were found to be: *max_features*: 0.5, *min_samples_leaf*: 3, and *n_estimators*: 150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's performance was as follows:\n",
    "\n",
    "- ***Accuracy*** (train) : 0.9826\n",
    "- ***F1 Score*** (train): 0.678998\n",
    "- ***Mean F1 Score*** (5-fold CV): 0.341827"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance was similar test set:\n",
    "- ***Accuracy*** (test): 0.9663\n",
    "- ***F1 Score*** (test): 0.346445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why is the F1 score so low?  Let's take a quick look at the confusion matrices for the training and test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong>Training</strong></center>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Predicted Negative</th>\n",
    "      <th>Predicted Positive</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Actual Negative</th>\n",
    "      <td>296480</td>\n",
    "      <td>5310</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Actual Positive</th>\n",
    "      <td>35</td>\n",
    "      <td>5653</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong>Test</strong></center>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Predicted Negative</th>\n",
    "      <th>Predicted Positive</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Actual Negative</th>\n",
    "      <td>73590</td>\n",
    "      <td>1857</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Actual Positive</th>\n",
    "      <td>735</td>\n",
    "      <td>687</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things immediately jump out:\n",
    "- First, recall is much better on the training set than the test set.  This suggests that some aspect of the positive examples in the test set was not captured in the training set.\n",
    "- Second, precision is lacking in the model's performance on both sets of data.  It is predicting a lot of negative examples as positive.\n",
    "\n",
    "Let's investigate the first issue by looking at which data points our algorithm is incorrectly classifying:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figs/recall_test.png\" alt=\"feature_importance\" style=\"height: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the map we can see that a significant number of the false negatives (actual blighted buildings predicted as non-blighted) are from areas where the training set didn't have much data.  This implies that collecting more geographically diverse examples of blighted buildings might improve performance, if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As to the second issue: we can get an idea of what features our model is weighting using a parameter called the [feature importance](https://en.wikipedia.org/wiki/Random_forest#Variable_importance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figs/feature_importance.png\" alt=\"feature_importance\" style=\"height: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the feature importance graph we can see that the only features getting a lot of weight are the tax status of the property, the coordinate location of the property, and the estimated sale value.  These features are present on every data point, so it makes sense that they would be more important to the algorithm, but still one would think that the blight violation features would be more predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further inspection reveals that the problem stems from our \"ground truth\" label of blight.  Lacking a better proxy for blightedness, we used city of Detroit demolition data to classify a building as blighted or not.  This leads to buildings that are actually blighted, but haven't been demolished yet, to be erroneously classified as non-blighted in our model.  In fact, after much digging, I couldn't find one building that was incorrectly identified as blighted by the algorithm that proved to be non-blighted upon visual inspection via Google Maps. For example, all of the following buildings were \"false positives\" that were ground-truth labeled as non-blighted using the demolitions data to define blightedness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figs/487_algonquin.png\" alt=\"\" style=\"height: 400px;\"/>\n",
    "<img src=\"files/figs/2961_basset.png\" alt=\"\" style=\"height: 400px;\"/>\n",
    "<img src=\"files/figs/5083_buckingham.png\" alt=\"\" style=\"height: 400px;\"/>\n",
    "<img src=\"files/figs/8213_fielding.png\" alt=\"\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms are only as good as the data you feed them.  For our model to have good performance, we would need a better indicator as to a building's blighted status than city demolitions data.  The best way to do this would be in-person site inspections to generate a training set.  Barring that, I would bet you could get a good indication of whether a building has been abandoned or not by analyzing its utilities data.  Since I don't have access to data for that, this is where the analysis ends for now. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
